{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f559fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Kaggle\\.ssh\\news_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# C√†i ƒë·∫∑t seed ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ nh·∫•t qu√°n\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b488ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0016bf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.40.0 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (2025.7.34)\n",
      "Requirement already satisfied: requests in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from transformers==4.40.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.14.1)\n",
      "Requirement already satisfied: colorama in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.40.0) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from requests->transformers==4.40.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from requests->transformers==4.40.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from requests->transformers==4.40.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\kaggle\\.ssh\\news_project\\.venv\\lib\\site-packages (from requests->transformers==4.40.0) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.40.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18294e",
   "metadata": {},
   "source": [
    "# ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc8361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original data shape: (8697, 10)\n",
      "Columns: ['text', 'label', 'id', 'user_name', 'post_message', 'timestamp_post', 'num_like_post', 'num_comment_post', 'num_share_post', 'clean_text']\n",
      "After dropping NaN: (4225, 10)\n",
      "After removing empty text: (4225, 10)\n",
      "Label distribution:\n",
      "label\n",
      "0    4225\n",
      "Name: count, dtype: int64\n",
      "Sample texts:\n",
      "Label 0: B√°c_sƒ© TP HCM th·∫Øng gi·∫£i nhi·∫øp_·∫£nh th·∫ø_gi·ªõi ƒê·∫ßu th√°ng 7 b√°c_sƒ© Ho√†i_Anh h√°o_h·ª©c ch·ªù ƒë√≥n chi·∫øc c√∫p Nh...\n",
      "Label 0: 11 c√°n_b·ªô ph·∫£i quay l·∫°i l√†m_vi·ªác sau n·ª≠a nƒÉm ngh·ªâ h∆∞u Ng√†y 276 l√£nh_ƒë·∫°o huy·ªán Qu·ª≥nh_L∆∞u cho bi·∫øt ƒë√£ ...\n",
      "Label 0: Th√™m h∆°n 210000 ng∆∞·ªùi Nga ƒëƒÉng_k√Ω tham_chi·∫øn ·ªü Ukraine_ƒê√¢y l√† th√†nh_qu·∫£ ph·ªëi_h·ª£p c·ªßa t·∫•t_c·∫£ c√°c c∆°_q...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('clean_news.csv')\n",
    "\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# B·ªè c√°c d√≤ng thi·∫øu d·ªØ li·ªáu\n",
    "df = df.dropna(subset=[\"clean_text\", \"label\"])\n",
    "print(f\"After dropping NaN: {df.shape}\")\n",
    "\n",
    "# √âp ki·ªÉu v√† lo·∫°i b·ªè d√≤ng r·ªóng\n",
    "df[\"clean_text\"] = df[\"clean_text\"].astype(str).str.strip()\n",
    "df = df[df[\"clean_text\"] != \"\"]\n",
    "print(f\"After removing empty text: {df.shape}\")\n",
    "\n",
    "# √âp ki·ªÉu nh√£n\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"Sample texts:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"Label {df.iloc[i]['label']}: {df.iloc[i]['clean_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71ac1c",
   "metadata": {},
   "source": [
    "# T√°ch d·ªØ li·ªáu train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a59fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4225\n",
      "Unique labels: {0}\n",
      "Train samples: 3380\n",
      "Validation samples: 845\n",
      "Train label distribution: {0: 3380}\n",
      "Validation label distribution: {0: 845}\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"clean_text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Unique labels: {set(labels)}\")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Train label distribution: {pd.Series(train_labels).value_counts().to_dict()}\")\n",
    "print(f\"Validation label distribution: {pd.Series(val_labels).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f3a28",
   "metadata": {},
   "source": [
    "# Tokenization v·ªõi PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924c7ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PhoBERT tokenizer...\n",
      "Tokenizing training data...\n",
      "Tokenizing validation data...\n",
      "Training encodings shape: torch.Size([3380, 256])\n",
      "Validation encodings shape: torch.Size([845, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading PhoBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "\n",
    "print(\"Tokenizing training data...\")\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=256, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_encodings = tokenizer(\n",
    "    val_texts, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=256, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"Training encodings shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Validation encodings shape: {val_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786b767",
   "metadata": {},
   "source": [
    "# T·∫°o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa5b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3380\n",
      "Validation dataset size: 845\n",
      "Sample keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Sample input_ids shape: torch.Size([256])\n",
      "Sample label: 0\n",
      "Sample label dtype: torch.int64\n",
      "Sample label shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item[\"labels\"] = label\n",
    "        return item\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Test m·ªôt sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Sample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample label: {sample['labels']}\")\n",
    "print(f\"Sample label dtype: {sample['labels'].dtype}\")\n",
    "print(f\"Sample label shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c1b97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26fe4c2e",
   "metadata": {},
   "source": [
    "# T·∫°o v√† c·∫•u h√¨nh m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e3f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d9a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 1\n",
      "Unique labels: [0]\n",
      "Loading PhoBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model parameters: 134,999,041\n",
      "Testing forward pass...\n",
      "Output shape: torch.Size([1, 1])\n",
      "Expected shape: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(set(labels))\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Unique labels: {sorted(set(labels))}\")\n",
    "\n",
    "print(\"Loading PhoBERT model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/phobert-base\", \n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "test_batch = next(iter(train_dataset))\n",
    "test_batch = {k: v.unsqueeze(0).to(device) for k, v in test_batch.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_batch)\n",
    "    print(f\"Output shape: {outputs.logits.shape}\")\n",
    "    print(f\"Expected shape: [1, {num_labels}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b622f08",
   "metadata": {},
   "source": [
    "# C·∫•u h√¨nh training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2664f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TrainingArguments ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainingArguments ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9072f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load l·∫°i tokenizer v√† model t·ª´ th∆∞ m·ª•c ƒë√£ l∆∞u\n",
    "model_dir = \"D:/Kaggle/.ssh/news_project/saved_model/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Thi·∫øt b·ªã hu·∫•n luy·ªán (GPU n·∫øu c√≥, kh√¥ng th√¨ CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# N·∫øu b·∫°n mu·ªën ti·∫øp t·ª•c train, th√¨ c·∫•u h√¨nh training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "# N·∫øu ch∆∞a t·∫°o eval_dataset th√¨ t·∫°o t·∫°i ƒë√¢y\n",
    "eval_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "# T·∫°o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Hu·∫•n luy·ªán ti·∫øp t·ª•c n·∫øu mu·ªën\n",
    "print(\"=== Ti·∫øp t·ª•c hu·∫•n luy·ªán b·∫±ng Trainer ===\")\n",
    "try:\n",
    "    trainer.train()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚õî ƒê√£ d·ª´ng gi·ªØa ch·ª´ng. L∆∞u model t·∫°m th·ªùi...\")\n",
    "    model.save_pretrained(\"saved_model_partial/\")\n",
    "    print(\"‚úÖ Hu·∫•n luy·ªán ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°m th·ªùi!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói Trainer: {e}\")\n",
    "    print(\"‚è≥ ƒêang chuy·ªÉn sang hu·∫•n luy·ªán th·ªß c√¥ng...\")\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.optim import AdamW\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/3\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch[\"labels\"] = batch[\"labels\"].long()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"üîÅ Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# ‚úÖ L∆∞u m√¥ h√¨nh cu·ªëi c√πng\n",
    "model.save_pretrained(\"saved_model/\")\n",
    "tokenizer.save_pretrained(\"saved_model/\")\n",
    "print(\"üíæ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i 'saved_model/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ed1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823ef125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load l·∫°i t·ª´ th∆∞ m·ª•c ƒë√£ l∆∞u\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"saved_model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"saved_model/\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f608274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"./saved_model\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (ch√∫ √Ω num_labels = 2 n·∫øu l√† b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff388ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    print(\"Max input_id:\", input_ids.max())\n",
    "    print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a693df0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47604bbe",
   "metadata": {},
   "source": [
    "# T·∫°o Trainer v√† hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ac5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CPU-optimized training...\n",
      "Training on 3380 samples\n",
      "Validation on 845 samples\n",
      "Estimated training time: 4.2 minutes\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 50/633 [12:59<2:36:56, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.415, 'grad_norm': 1.2937830686569214, 'learning_rate': 5e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 100/633 [24:13<1:55:28, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0285, 'grad_norm': 0.13045376539230347, 'learning_rate': 1e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñé       | 150/633 [35:02<1:42:11, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0062, 'grad_norm': 0.07929304242134094, 'learning_rate': 9.061913696060039e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 200/633 [45:54<1:35:56, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'grad_norm': 0.05073023587465286, 'learning_rate': 8.123827392120077e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 250/633 [56:33<1:18:27, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'grad_norm': 0.03435263782739639, 'learning_rate': 7.185741088180113e-06, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 300/633 [1:06:49<1:08:25, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'grad_norm': 0.02988443896174431, 'learning_rate': 6.2476547842401506e-06, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 350/633 [1:17:06<57:45, 12.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0019, 'grad_norm': 0.0271089356392622, 'learning_rate': 5.309568480300188e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 400/633 [1:27:19<47:39, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'grad_norm': 0.023642314597964287, 'learning_rate': 4.3714821763602255e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 450/633 [1:37:30<37:11, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'grad_norm': 0.020636925473809242, 'learning_rate': 3.4333958724202633e-06, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 500/633 [1:47:42<27:21, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'grad_norm': 0.019087564200162888, 'learning_rate': 2.4953095684803003e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 500/633 [1:50:23<27:21, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008740455377846956, 'eval_runtime': 161.2582, 'eval_samples_per_second': 5.24, 'eval_steps_per_second': 1.315, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 550/633 [2:00:41<16:54, 12.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'grad_norm': 0.018626516684889793, 'learning_rate': 1.557223264540338e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 600/633 [2:10:52<06:42, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'grad_norm': 0.018596813082695007, 'learning_rate': 6.191369606003752e-07, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 633/633 [2:17:37<00:00, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8257.6918, 'train_samples_per_second': 1.228, 'train_steps_per_second': 0.077, 'train_loss': 0.036960163028648865, 'epoch': 3.0}\n",
      "Training completed in 137.6 minutes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training v·ªõi progress tracking cho CPU\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"Starting CPU-optimized training...\")\n",
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "print(f\"Validation on {len(val_dataset)} samples\")\n",
    "\n",
    "# ∆Ø·ªõc t√≠nh th·ªùi gian\n",
    "estimated_time_per_epoch = len(train_dataset) / 4 * 0.1  # Gi·∫£ s·ª≠ 0.1s per batch\n",
    "total_estimated_time = estimated_time_per_epoch * 3 / 60\n",
    "print(f\"Estimated training time: {total_estimated_time:.1f} minutes\")\n",
    "\n",
    "# Training v·ªõi Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting training...\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    print(f\"Training completed in {training_time:.1f} minutes!\")\n",
    "    \n",
    "    # ‚úÖ L∆∞u m√¥ h√¨nh sau khi train th√†nh c√¥ng\n",
    "    trainer.save_model(\"models/final_model\")\n",
    "    tokenizer.save_pretrained(\"models/final_model\")\n",
    "    print(\"Model saved to models/final_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"Falling back to manual training...\")\n",
    "\n",
    "    # Manual training fallback\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.optim import AdamW\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/3\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch['labels'] = batch['labels'].long()\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # ‚úÖ L∆∞u m√¥ h√¨nh sau hu·∫•n luy·ªán th·ªß c√¥ng\n",
    "    model_save_path = \"models/manual_trained_model\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"{model_save_path}/pytorch_model.bin\")\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Manual training model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58606a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 64000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ token n√†o v∆∞·ª£t qu√° vocab kh√¥ng\n",
    "for batch in train_dataset:\n",
    "    if max(batch[\"input_ids\"]) >= tokenizer.vocab_size:\n",
    "        print(\"Found invalid token id:\", max(batch[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataset:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    if input_ids.max() >= model.config.vocab_size:\n",
    "        print(\"C√≥ token v∆∞·ª£t qu√° vocab_size!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a1aa4",
   "metadata": {},
   "source": [
    "# ƒê√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(trainer))  # K·∫øt qu·∫£ ph·∫£i l√† <class 'transformers.trainer.Trainer'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1573634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [02:40<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008740455377846956, 'eval_runtime': 161.6071, 'eval_samples_per_second': 5.229, 'eval_steps_per_second': 1.312, 'epoch': 2.996449704142012}\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [02:38<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       845\n",
      "\n",
      "    accuracy                           1.00       845\n",
      "   macro avg       1.00      1.00      1.00       845\n",
      "weighted avg       1.00      1.00      1.00       845\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[845]]\n",
      "\n",
      "Saving model...\n",
      "Model saved to ./best_model/\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "\n",
    "# ƒê√°nh gi√°\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "print(\"Making predictions...\")\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# T√≠nh metrics\n",
    "accuracy = accuracy_score(val_labels, pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, pred_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(val_labels, pred_labels)\n",
    "print(cm)\n",
    "\n",
    "# L∆∞u model\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(\"./best_model\")\n",
    "tokenizer.save_pretrained(\"./best_model\")\n",
    "print(\"Model saved to ./best_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecec2917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved to ./best_model/\n",
      "Testing model loading...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(\"./best_model\")\n",
    "tokenizer.save_pretrained(\"./best_model\")\n",
    "print(\"Model saved to ./best_model/\")\n",
    "\n",
    "# Test loading model\n",
    "print(\"Testing model loading...\")\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382740be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing saved model...\n",
      "Test text: Al Nassr l·∫ßn th·ª© t∆∞ thay HLV t·ª´ khi c√≥ Ronaldo_Al Nassr th√¥ng_b√°o Pioli v√† ƒë·ªôi_ng≈© tr·ª£_l√Ω c·ªßa √¥ng s·∫Ω...\n",
      "Predicted label: 0\n",
      "Confidence: 0.9992\n",
      "True label: 0\n",
      "Prediction correct: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Test model ƒë√£ train\n",
    "print(\"Testing saved model...\")\n",
    "\n",
    "# Load model ƒë√£ train\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "\n",
    "# Test v·ªõi m·ªôt sample\n",
    "if len(val_texts) > 0:\n",
    "    test_text = val_texts[0]\n",
    "    print(f\"Test text: {test_text[:100]}...\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = loaded_tokenizer(\n",
    "        test_text, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=256, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(f\"True label: {val_labels[0]}\")\n",
    "    print(f\"Prediction correct: {predicted_label == val_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56fdc846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction function:\n",
      "Text 1: Tin t·ª©c m·ªõi nh·∫•t v·ªÅ t√¨nh h√¨nh kinh t·∫ø Vi·ªát Nam...\n",
      "  Predicted: 0, Confidence: 0.9982\n",
      "\n",
      "Text 2: C·∫≠p nh·∫≠t v·ªÅ d·ªãch b·ªánh COVID-19 t·∫°i H√† N·ªôi...\n",
      "  Predicted: 0, Confidence: 0.9986\n",
      "\n",
      "Text 3: K·∫øt qu·∫£ tr·∫≠n ƒë·∫•u b√≥ng ƒë√° t·ªëi qua...\n",
      "  Predicted: 0, Confidence: 0.9983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_news(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n nh√£n cho m·ªôt ƒëo·∫°n text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=256, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Chuy·ªÉn l√™n device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "        predicted_label = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_label].item()\n",
    "    \n",
    "    return predicted_label, confidence\n",
    "\n",
    "# Test h√†m d·ª± ƒëo√°n\n",
    "test_texts = [\n",
    "    \"Tin t·ª©c m·ªõi nh·∫•t v·ªÅ t√¨nh h√¨nh kinh t·∫ø Vi·ªát Nam\",\n",
    "    \"C·∫≠p nh·∫≠t v·ªÅ d·ªãch b·ªánh COVID-19 t·∫°i H√† N·ªôi\",\n",
    "    \"K·∫øt qu·∫£ tr·∫≠n ƒë·∫•u b√≥ng ƒë√° t·ªëi qua\"\n",
    "]\n",
    "\n",
    "print(\"Testing prediction function:\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    predicted_label, confidence = predict_news(text, loaded_model, loaded_tokenizer, device)\n",
    "    print(f\"Text {i+1}: {text[:50]}...\")\n",
    "    print(f\"  Predicted: {predicted_label}, Confidence: {confidence:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
