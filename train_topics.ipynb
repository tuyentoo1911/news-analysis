{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huấn luyện mô hình chủ đề (LDA) từ `merged.json`\n",
        "\n",
        "Notebook này giúp bạn:\n",
        "- Nạp dữ liệu từ `merged.json`, tiền xử lý văn bản\n",
        "- Train LDA với `scikit-learn`\n",
        "- Lưu model (`lda_model.joblib`), vectorizer (`vectorizer_bow.joblib`)\n",
        "- Xuất chủ đề (`topics.json`, `topics.txt`) và gán chủ đề cho từng bài (`articles_with_topics.csv`)\n",
        "- Tùy chọn sinh tóm tắt extractive cho mỗi chủ đề\n",
        "\n",
        "Chạy lần lượt các cell bên dưới. Có thể chỉnh các tham số trong cell Cấu hình.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cài đặt thư viện nếu thiếu (chỉ chạy nếu cần)\n",
        "# !pip install pandas scikit-learn beautifulsoup4 joblib\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Cấu hình\n",
        "DATA_PATH = \"merged.json\"\n",
        "OUTPUT_DIR = \"models\"\n",
        "NUM_TOPICS = 12\n",
        "MAX_FEATURES = 30000\n",
        "MIN_DF = 3\n",
        "MAX_DF = 0.9\n",
        "RANDOM_STATE = 42\n",
        "TOP_WORDS = 15\n",
        "SUMMARIZE_TOPICS = True\n",
        "TOPIC_SUMMARY_SENTENCES = 5\n",
        "TOPIC_SUMMARY_MAX_ARTICLES = 200\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7050, 7)\n"
          ]
        }
      ],
      "source": [
        "def strip_html(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text(separator=\" \")\n",
        "\n",
        "\n",
        "def basic_clean(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = strip_html(text)\n",
        "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
        "    text = re.sub(r\"www\\.\\S+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def load_articles(path: str) -> pd.DataFrame:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    rows = []\n",
        "    for item in data:\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "        title = item.get(\"title\") or item.get(\"headline\") or item.get(\"name\")\n",
        "        desc = item.get(\"description\") or item.get(\"summary\")\n",
        "        content = item.get(\"content\") or item.get(\"body\") or item.get(\"text\")\n",
        "        url = item.get(\"url\") or item.get(\"link\")\n",
        "        category = item.get(\"category\") or item.get(\"section\") or item.get(\"cat\")\n",
        "        published = (\n",
        "            item.get(\"publishedAt\")\n",
        "            or item.get(\"pubDate\")\n",
        "            or item.get(\"date\")\n",
        "            or item.get(\"created_at\")\n",
        "        )\n",
        "        full_text = \" \".join([str(title or \"\"), str(desc or \"\"), str(content or \"\")]).strip()\n",
        "        rows.append(\n",
        "            {\n",
        "                \"title\": title,\n",
        "                \"description\": desc,\n",
        "                \"content\": content,\n",
        "                \"url\": url,\n",
        "                \"category\": category,\n",
        "                \"published\": published,\n",
        "                \"text\": basic_clean(full_text),\n",
        "            }\n",
        "        )\n",
        "    df = pd.DataFrame(rows)\n",
        "    df = df[df[\"text\"].astype(str).str.len() > 0].reset_index(drop=True)\n",
        "    df.head(3)\n",
        "    return df\n",
        "\n",
        "\n",
        "df = load_articles(DATA_PATH)\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chủ đề 0:  bệnh, thể, các, cơ, người, không, chất, khi, hoặc, ăn, là, sĩ, cho, thường, bác\n",
            "Chủ đề 1:  mỹ, ông, cho, không, đã, một, các, là, với, này, khi, thống, được, trump, ngày\n",
            "Chủ đề 2:  không, là, bị, án, đề, được, cho, một, câu, các, với, học, để, này, đã\n",
            "Chủ đề 3:  công, các, người, ai, cho, một, thể, dụng, với, được, động, hình, trên, năng, ra\n",
            "Chủ đề 4:  trận, đội, bóng, thủ, đấu, là, với, khi, hai, anh, giải, thắng, sân, một, kết\n",
            "Chủ đề 5:  tôi, không, người, một, là, con, khi, nhưng, để, làm, cho, nhà, anh, lại, vì\n",
            "Chủ đề 6:  với, là, cho, giá, các, hơn, nước, không, độ, một, được, đồng, từ, bản, khi\n",
            "Chủ đề 7:  học, sinh, thi, trường, điểm, đại, năm, các, là, viên, với, công, thí, giáo, được\n",
            "Chủ đề 8:  xe, người, bị, đường, một, khi, trên, an, máy, đi, không, vào, sau, sát, đã\n",
            "Chủ đề 9:  công, các, doanh, chính, cho, với, hàng, đồng, định, được, là, đầu, số, kinh, năm\n",
            "Chủ đề 10:  khách, là, du, các, được, ảnh, với, đến, từ, một, những, năm, diễn, người, nhiều\n",
            "Chủ đề 11:  tỉnh, thành, công, bay, các, hành, đồng, quốc, bộ, trung, tại, tàu, từ, năm, được\n"
          ]
        }
      ],
      "source": [
        "# Vector hóa và huấn luyện LDA\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=MAX_FEATURES,\n",
        "    min_df=MIN_DF,\n",
        "    max_df=MAX_DF,\n",
        ")\n",
        "X_bow = vectorizer.fit_transform(df[\"text\"])  # sparse matrix\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=NUM_TOPICS,\n",
        "    learning_method=\"batch\",\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "W = lda.fit_transform(X_bow)\n",
        "H = lda.components_\n",
        "\n",
        "terms = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "def top_words_for_topic(topic_idx: int, n: int = TOP_WORDS) -> List[str]:\n",
        "    idx = np.argsort(H[topic_idx])[::-1][:n]\n",
        "    return terms[idx].tolist()\n",
        "\n",
        "for k in range(NUM_TOPICS):\n",
        "    print(f\"Chủ đề {k}: \", \", \".join(top_words_for_topic(k)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã lưu model, vectorizer và topics vào: d:\\Project\\cc\\models\n"
          ]
        }
      ],
      "source": [
        "# Lưu model và vectorizer\n",
        "joblib.dump(vectorizer, os.path.join(OUTPUT_DIR, \"vectorizer_bow.joblib\"))\n",
        "joblib.dump(lda, os.path.join(OUTPUT_DIR, \"lda_model.joblib\"))\n",
        "\n",
        "# Lưu danh sách từ khóa mỗi chủ đề\n",
        "topics = {}\n",
        "for i in range(NUM_TOPICS):\n",
        "    topics[str(i)] = top_words_for_topic(i, TOP_WORDS)\n",
        "with open(os.path.join(OUTPUT_DIR, \"topics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(topics, f, ensure_ascii=False, indent=2)\n",
        "with open(os.path.join(OUTPUT_DIR, \"topics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(NUM_TOPICS):\n",
        "        f.write(f\"Topic {i}: \")\n",
        "        f.write(\", \".join(top_words_for_topic(i, TOP_WORDS)))\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(\"Đã lưu model, vectorizer và topics vào:\", os.path.abspath(OUTPUT_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã lưu: models\\articles_with_topics.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>content</th>\n",
              "      <th>url</th>\n",
              "      <th>category</th>\n",
              "      <th>published</th>\n",
              "      <th>text</th>\n",
              "      <th>dominant_topic</th>\n",
              "      <th>dominant_topic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cao thủ duy nhất trong Kim Dung chết do võ côn...</td>\n",
              "      <td>Đây là một nhân vật đầy quyền lực và mưu mô tr...</td>\n",
              "      <td>Ông là giáo chủ đầy tham vọng của Nhật Nguyệt ...</td>\n",
              "      <td>https://vnexpress.net/crossword-giai-o-chu-o-c...</td>\n",
              "      <td>cuoi</td>\n",
              "      <td>None</td>\n",
              "      <td>Cao thủ duy nhất trong Kim Dung chết do võ côn...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.606936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ảnh phòng tắm có điểm sai duy nhất nào?</td>\n",
              "      <td>Chỉ với 30 giây bạn có nhận ra điểm thiếu sót ...</td>\n",
              "      <td>Trong bức ảnh miêu tả căn phòng tắm với đầy đủ...</td>\n",
              "      <td>https://vnexpress.net/cau-do-iq-thu-tai-tinh-m...</td>\n",
              "      <td>cuoi</td>\n",
              "      <td>None</td>\n",
              "      <td>Ảnh phòng tắm có điểm sai duy nhất nào? Chỉ vớ...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.492050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Triều đại cuối cùng đóng đô tại hai kinh đô kh...</td>\n",
              "      <td>Triều đại này tồn tại khoảng 24 năm, từng đóng...</td>\n",
              "      <td>Trước khi sụp đổ, triều đại này còn dự định đó...</td>\n",
              "      <td>https://vnexpress.net/crossword-giai-o-chu-o-c...</td>\n",
              "      <td>cuoi</td>\n",
              "      <td>None</td>\n",
              "      <td>Triều đại cuối cùng đóng đô tại hai kinh đô kh...</td>\n",
              "      <td>10</td>\n",
              "      <td>0.429586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Cao thủ duy nhất trong Kim Dung chết do võ côn...   \n",
              "1            Ảnh phòng tắm có điểm sai duy nhất nào?   \n",
              "2  Triều đại cuối cùng đóng đô tại hai kinh đô kh...   \n",
              "\n",
              "                                         description  \\\n",
              "0  Đây là một nhân vật đầy quyền lực và mưu mô tr...   \n",
              "1  Chỉ với 30 giây bạn có nhận ra điểm thiếu sót ...   \n",
              "2  Triều đại này tồn tại khoảng 24 năm, từng đóng...   \n",
              "\n",
              "                                             content  \\\n",
              "0  Ông là giáo chủ đầy tham vọng của Nhật Nguyệt ...   \n",
              "1  Trong bức ảnh miêu tả căn phòng tắm với đầy đủ...   \n",
              "2  Trước khi sụp đổ, triều đại này còn dự định đó...   \n",
              "\n",
              "                                                 url category published  \\\n",
              "0  https://vnexpress.net/crossword-giai-o-chu-o-c...     cuoi      None   \n",
              "1  https://vnexpress.net/cau-do-iq-thu-tai-tinh-m...     cuoi      None   \n",
              "2  https://vnexpress.net/crossword-giai-o-chu-o-c...     cuoi      None   \n",
              "\n",
              "                                                text  dominant_topic  \\\n",
              "0  Cao thủ duy nhất trong Kim Dung chết do võ côn...               2   \n",
              "1  Ảnh phòng tắm có điểm sai duy nhất nào? Chỉ vớ...               2   \n",
              "2  Triều đại cuối cùng đóng đô tại hai kinh đô kh...              10   \n",
              "\n",
              "   dominant_topic_score  \n",
              "0              0.606936  \n",
              "1              0.492050  \n",
              "2              0.429586  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gán chủ đề trội cho từng bài và lưu CSV\n",
        "\n",
        "dominant_topic = W.argmax(axis=1)\n",
        "max_topic_score = W.max(axis=1)\n",
        "\n",
        "df_topics = df.copy()\n",
        "df_topics[\"dominant_topic\"] = dominant_topic\n",
        "[df_topics.__setitem__(\"dominant_topic_score\", max_topic_score)]\n",
        "\n",
        "cols = [\"title\", \"url\", \"category\", \"published\", \"dominant_topic\", \"dominant_topic_score\"]\n",
        "for c in cols:\n",
        "    if c not in df_topics.columns:\n",
        "        df_topics[c] = None\n",
        "\n",
        "out_csv = os.path.join(OUTPUT_DIR, \"articles_with_topics.csv\")\n",
        "df_topics[cols].to_csv(out_csv, index=False)\n",
        "print(\"Đã lưu:\", out_csv)\n",
        "\n",
        "df_topics.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã lưu tóm tắt chủ đề vào: models\\topic_summaries.txt\n"
          ]
        }
      ],
      "source": [
        "# Tóm tắt extractive cho mỗi chủ đề (tùy chọn)\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    sentences = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
        "    return [s.strip() for s in sentences if s and len(s.strip()) > 3]\n",
        "\n",
        "\n",
        "def summarize_text(text: str, max_sentences: int = 5) -> str:\n",
        "    sentences = split_sentences(text)\n",
        "    if len(sentences) <= max_sentences:\n",
        "        return \" \".join(sentences)\n",
        "    vec = TfidfVectorizer(max_features=MAX_FEATURES)\n",
        "    tfidf = vec.fit_transform(sentences)\n",
        "    sim = cosine_similarity(tfidf)\n",
        "    n = sim.shape[0]\n",
        "    scores = np.ones(n) / n\n",
        "    damping = 0.85\n",
        "    for _ in range(30):\n",
        "        denom = np.maximum(sim.sum(axis=1), 1e-9)\n",
        "        new_scores = (1 - damping) / n + damping * sim.dot(scores) / denom\n",
        "        if np.allclose(new_scores, scores, atol=1e-6):\n",
        "            break\n",
        "        scores = new_scores\n",
        "    top_idx = np.argsort(scores)[::-1][:max_sentences]\n",
        "    top_idx = sorted(top_idx)\n",
        "    return \" \".join(sentences[i] for i in top_idx)\n",
        "\n",
        "\n",
        "if SUMMARIZE_TOPICS:\n",
        "    lines = []\n",
        "    for k in sorted(df_topics[\"dominant_topic\"].unique()):\n",
        "        subset = df_topics[df_topics[\"dominant_topic\"] == k]\n",
        "        texts = subset[\"text\"].head(TOPIC_SUMMARY_MAX_ARTICLES).tolist()\n",
        "        combined = \". \".join(texts)\n",
        "        summary = summarize_text(combined, max_sentences=TOPIC_SUMMARY_SENTENCES)\n",
        "        lines.append(f\"==== TOPIC {k} ====\")\n",
        "        lines.append(summary)\n",
        "        lines.append(\"\")\n",
        "\n",
        "    with open(os.path.join(OUTPUT_DIR, \"topic_summaries.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    print(\"Đã lưu tóm tắt chủ đề vào:\", os.path.join(OUTPUT_DIR, \"topic_summaries.txt\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiêu đề: Cao thủ duy nhất trong Kim Dung chết do võ công mình tạo ra?\n",
            "URL: https://vnexpress.net/crossword-giai-o-chu-o-chu-cao-thu-duy-nhat-trong-kim-dung-chet-do-vo-cong-minh-tao-ra-4905407.html\n",
            "\n",
            "Tóm tắt:\n",
            "Tuy nhiên, chính tuyệt kỹ này lại trở thành nguyên nhân dẫn đến cái chết bất ngờ của ông. Gợi ý:Ông là cha của Nhậm Doanh Doanh, tên đầy đủ có 11 chữ cái, bắt đầu là chữ N và kết thúc là chữ H. >> Xem video hướng dẫn cách giải ô chữ tại đây >> Xem đáp án Hi\n"
          ]
        }
      ],
      "source": [
        "# Tóm tắt 1 bài theo chỉ số (index)\n",
        "# Đổi giá trị idx để chọn bài cần tóm tắt\n",
        "idx = 0  # ví dụ: bài đầu tiên\n",
        "\n",
        "if 0 <= idx < len(df):\n",
        "    print(\"Tiêu đề:\", df.loc[idx, \"title\"])\n",
        "    print(\"URL:\", df.loc[idx, \"url\"])\n",
        "    summary_single = summarize_text(df.loc[idx, \"text\"], max_sentences=3)\n",
        "    print(\"\\nTóm tắt:\")\n",
        "    print(summary_single)\n",
        "else:\n",
        "    print(\"idx ngoài phạm vi dữ liệu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== TÓM TẮT CHỦ ĐỀ 0 ====\n",
            "Những thứ không xả xuống bồn cầu? Tuy nhiên, nếu hộp không chịu được nhiệt cao (thường là hộp nhựa thông thường, không chuyên dụng cho nhiệt), việc tích nhiệt và áp suất lâu bên trong có thể gây ra hiện tượng biến dạng, bung nắp, hoặc xuất hiện các vết xước nhỏ ở đáy – nơi dễ phát sinh vi khuẩn khi tái sử dụng. Kíp cấp cứu đã đến bến tàu, tiếp cận người bệnh vào khoảng 9h50 trong tình trạng đang được cấp cứu hồi sinh tim phổi cơ bản, vùng hầu họng nhiều thức ăn và dịch tiêu hóa, mạch cánh đập rời rạc. Anna Strasma, bác sĩ tại Đại học Duke, cho rằng CKDu có thể là tập hợp các bệnh với nhiều nguyên nhân khác nhau. Ảnh:Dreamstime Các nhà nghiên cứu thừa nhận một số hạn chế trong nghiên cứu, như khả năng không đại diện cho các nhóm dân số khác và các yếu tố nguy cơ khác như hút thuốc có thể ảnh hưởng đến kết quả.\n"
          ]
        }
      ],
      "source": [
        "# Tóm tắt nhanh toàn bộ 1 chủ đề theo topic_id\n",
        "# Đổi topic_id để chọn chủ đề cần xem tóm tắt\n",
        "\n",
        "def summarize_topic(topic_id: int, max_sentences: int = 5, max_articles: int = 200) -> str:\n",
        "    subset = df_topics[df_topics[\"dominant_topic\"] == topic_id]\n",
        "    if subset.empty:\n",
        "        return \"(Không có bài phù hợp)\"\n",
        "    texts = subset[\"text\"].head(max_articles).tolist()\n",
        "    combined = \". \".join(texts)\n",
        "    return summarize_text(combined, max_sentences=max_sentences)\n",
        "\n",
        "# ví dụ: tóm tắt chủ đề 0\n",
        "topic_id = 0\n",
        "print(f\"==== TÓM TẮT CHỦ ĐỀ {topic_id} ====\")\n",
        "print(summarize_topic(topic_id, max_sentences=5))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
